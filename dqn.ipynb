{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5490cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Environment\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "from algorithms import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a625d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state(state):\n",
    "    ret_state = {}\n",
    "    # state[\"time_step\"] = np.array([state[\"time_step\"]]).astype(np.float32).flatten(0)\n",
    "    # state[\"map\"] = np.array(state[\"map\"]).astype(np.float32)\n",
    "    ret_state[\"robots\"] = np.array(state[\"robots\"]).astype(np.float32).flatten()\n",
    "    ret_state[\"packages\"] = np.array(state[\"packages\"]).astype(np.float32).flatten()[:100]\n",
    "    if len(ret_state[\"packages\"]) < 1000:\n",
    "        ret_state[\"packages\"] = np.concatenate((ret_state[\"packages\"], np.zeros(100-len(ret_state[\"packages\"]))))\n",
    "    return np.concatenate(list(ret_state.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edfc0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_shaping(r, env, state, action):\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61fdb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(gym.Env):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Env, self).__init__()\n",
    "        self.env = Environment(*args, **kwargs)\n",
    "\n",
    "        self.action_space = spaces.multi_discrete.MultiDiscrete([5, 3]*self.env.n_robots)\n",
    "\n",
    "        self.prev_state = self.env.reset()\n",
    "        first_state=convert_state(self.prev_state)\n",
    "        # Define observation space as a dictionary\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=100, shape=first_state.shape, dtype=np.float32)\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        self.le1, self.le2= LabelEncoder(), LabelEncoder()\n",
    "        self.le1.fit(['S', 'L', 'R', 'U', 'D'])\n",
    "        self.le2.fit(['0','1', '2'])\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.prev_state = self.env.reset()\n",
    "        return convert_state(self.prev_state), {}\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render()\n",
    "\n",
    "    def step(self, action):\n",
    "        ret = []\n",
    "        ret.append(self.le1.inverse_transform(action.reshape(-1, 2).T[0]))\n",
    "        ret.append(self.le2.inverse_transform(action.reshape(-1, 2).T[1]))\n",
    "        action = list(zip(*ret))\n",
    "\n",
    "        # You should not modify the infos object\n",
    "        s, r, done, infos = self.env.step(action)\n",
    "        new_r = reward_shaping(r, self.env, self.prev_state, action)\n",
    "        self.prev_state = s\n",
    "        return convert_state(s), new_r, \\\n",
    "            done, False, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01987daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, num_episode=10):\n",
    "    total_rewards = []\n",
    "\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0 \n",
    "    \n",
    "    for _ in range(num_episode):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "            action = agent.select_action(state_tensor)\n",
    "            next_state, reward, done, _, infos = env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "        ep_reward = infos.get(\"total_reward\", reward)\n",
    "        total_rewards.append(ep_reward)\n",
    "\n",
    "    agent.epsilon = original_epsilon\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Evaluation reward over {num_episode} episodes: {avg_reward}\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c543e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards(rewards, filename=\"dqn_learning_curve.png\", window=10):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    rewards = np.array(rewards)\n",
    "    averages = np.convolve(rewards, np.ones(window)/window, mode=\"valid\")\n",
    "    plt.plot(rewards, label=\"Reward per episode\")\n",
    "    plt.plot(range(window-1, len(rewards)), averages, label=f\"{window}-episode average\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.legend()\n",
    "    plt.title(\"DQN Learning Curve\")\n",
    "    plt.grid()\n",
    "    plt.savefig(f\"plots/{filename}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b21803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(agent, filename=\"dqn\", map_name=\"map\"):\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    torch.save(agent.policy_net.state_dict(), f\"models/{filename}_{map_name}.pth\")\n",
    "    print(f\"Model saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d5353e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(agent, filename=\"dqn_model.pth\"):\n",
    "    agent.policy_net.load_state_dict(torch.load(f\"models/{filename}\", map_location=agent.device))\n",
    "    agent.target_net.load_state_dict(agent.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71278421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67c4b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(agent, env,\n",
    "              map_name=\"map\",\n",
    "              num_episodes=500,\n",
    "              target_update=10,\n",
    "              eval_every=10, seed=42):\n",
    "    set_seed(seed)\n",
    "    rewards_history = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "            action = agent.select_action(state_tensor)\n",
    "            next_state, reward, done, _, infos = env.step(action)\n",
    "\n",
    "            agent.buffer_push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            agent.update()\n",
    "        \n",
    "        # Use total_reward from infos\n",
    "        total_reward = infos.get(\"total_reward\", reward)\n",
    "        rewards_history.append(total_reward)\n",
    "\n",
    "        if ep % target_update == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        if eval_every and ep % eval_every == 0:\n",
    "            avg_reward = evaluate_agent(agent, env)\n",
    "            print(f\"Episode {ep}, Average Evaluation Reward: {avg_reward}\")\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    save_model(agent, filename=\"dqn\", map_name=map_name)\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca474021",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7e970f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "map_name = \"map\"\n",
    "mapfile = map_name + \".txt\"\n",
    "env = Env(mapfile, seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd48e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.shape[0]\n",
    "n_robots = env.env.n_robots\n",
    "agent = DQNAgent(obs_size=obs_size, n_robots=n_robots)\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ab0f63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation reward over 10 episodes: -0.068\n",
      "Episode 0, Average Evaluation Reward: -0.068\n",
      "Average Evaluation reward over 10 episodes: -0.076\n",
      "Episode 10, Average Evaluation Reward: -0.076\n",
      "Average Evaluation reward over 10 episodes: -0.5980000000000004\n",
      "Episode 20, Average Evaluation Reward: -0.5980000000000004\n",
      "Average Evaluation reward over 10 episodes: 1.530000000000001\n",
      "Episode 30, Average Evaluation Reward: 1.530000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.2340000000000003\n",
      "Episode 40, Average Evaluation Reward: -0.2340000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.3730000000000004\n",
      "Episode 50, Average Evaluation Reward: -0.3730000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.37700000000000033\n",
      "Episode 60, Average Evaluation Reward: -0.37700000000000033\n",
      "Average Evaluation reward over 10 episodes: -0.41000000000000014\n",
      "Episode 70, Average Evaluation Reward: -0.41000000000000014\n",
      "Average Evaluation reward over 10 episodes: -0.24800000000000036\n",
      "Episode 80, Average Evaluation Reward: -0.24800000000000036\n",
      "Average Evaluation reward over 10 episodes: 1.4190000000000025\n",
      "Episode 90, Average Evaluation Reward: 1.4190000000000025\n",
      "Average Evaluation reward over 10 episodes: 1.5640000000000012\n",
      "Episode 100, Average Evaluation Reward: 1.5640000000000012\n",
      "Average Evaluation reward over 10 episodes: -0.6880000000000003\n",
      "Episode 110, Average Evaluation Reward: -0.6880000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.6220000000000007\n",
      "Episode 120, Average Evaluation Reward: 0.6220000000000007\n",
      "Average Evaluation reward over 10 episodes: -0.4070000000000002\n",
      "Episode 130, Average Evaluation Reward: -0.4070000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.6220000000000009\n",
      "Episode 140, Average Evaluation Reward: 0.6220000000000009\n",
      "Average Evaluation reward over 10 episodes: 3.8739999999999966\n",
      "Episode 150, Average Evaluation Reward: 3.8739999999999966\n",
      "Average Evaluation reward over 10 episodes: -0.5910000000000004\n",
      "Episode 160, Average Evaluation Reward: -0.5910000000000004\n",
      "Average Evaluation reward over 10 episodes: 1.4540000000000013\n",
      "Episode 170, Average Evaluation Reward: 1.4540000000000013\n",
      "Average Evaluation reward over 10 episodes: 0.6560000000000018\n",
      "Episode 180, Average Evaluation Reward: 0.6560000000000018\n",
      "Average Evaluation reward over 10 episodes: 1.652999999999998\n",
      "Episode 190, Average Evaluation Reward: 1.652999999999998\n",
      "Average Evaluation reward over 10 episodes: -0.3780000000000002\n",
      "Episode 200, Average Evaluation Reward: -0.3780000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.1300000000000004\n",
      "Episode 210, Average Evaluation Reward: -0.1300000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.8360000000000001\n",
      "Episode 220, Average Evaluation Reward: 0.8360000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.4280000000000004\n",
      "Episode 230, Average Evaluation Reward: -0.4280000000000004\n",
      "Average Evaluation reward over 10 episodes: 2.6410000000000027\n",
      "Episode 240, Average Evaluation Reward: 2.6410000000000027\n",
      "Average Evaluation reward over 10 episodes: 0.5530000000000002\n",
      "Episode 250, Average Evaluation Reward: 0.5530000000000002\n",
      "Average Evaluation reward over 10 episodes: 2.580000000000001\n",
      "Episode 260, Average Evaluation Reward: 2.580000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.3690000000000003\n",
      "Episode 270, Average Evaluation Reward: -0.3690000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.5200000000000007\n",
      "Episode 280, Average Evaluation Reward: 0.5200000000000007\n",
      "Average Evaluation reward over 10 episodes: 2.4810000000000025\n",
      "Episode 290, Average Evaluation Reward: 2.4810000000000025\n",
      "Average Evaluation reward over 10 episodes: 0.5970000000000001\n",
      "Episode 300, Average Evaluation Reward: 0.5970000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.32900000000000024\n",
      "Episode 310, Average Evaluation Reward: -0.32900000000000024\n",
      "Average Evaluation reward over 10 episodes: 0.6670000000000013\n",
      "Episode 320, Average Evaluation Reward: 0.6670000000000013\n",
      "Average Evaluation reward over 10 episodes: 0.6570000000000004\n",
      "Episode 330, Average Evaluation Reward: 0.6570000000000004\n",
      "Average Evaluation reward over 10 episodes: 2.863000000000002\n",
      "Episode 340, Average Evaluation Reward: 2.863000000000002\n",
      "Average Evaluation reward over 10 episodes: 2.4670000000000023\n",
      "Episode 350, Average Evaluation Reward: 2.4670000000000023\n",
      "Average Evaluation reward over 10 episodes: 1.6570000000000011\n",
      "Episode 360, Average Evaluation Reward: 1.6570000000000011\n",
      "Average Evaluation reward over 10 episodes: -0.18300000000000033\n",
      "Episode 370, Average Evaluation Reward: -0.18300000000000033\n",
      "Average Evaluation reward over 10 episodes: 0.49500000000000055\n",
      "Episode 380, Average Evaluation Reward: 0.49500000000000055\n",
      "Average Evaluation reward over 10 episodes: 0.6780000000000014\n",
      "Episode 390, Average Evaluation Reward: 0.6780000000000014\n",
      "Average Evaluation reward over 10 episodes: -0.44700000000000023\n",
      "Episode 400, Average Evaluation Reward: -0.44700000000000023\n",
      "Average Evaluation reward over 10 episodes: 1.3700000000000023\n",
      "Episode 410, Average Evaluation Reward: 1.3700000000000023\n",
      "Average Evaluation reward over 10 episodes: 0.5950000000000001\n",
      "Episode 420, Average Evaluation Reward: 0.5950000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.029000000000000192\n",
      "Episode 430, Average Evaluation Reward: -0.029000000000000192\n",
      "Average Evaluation reward over 10 episodes: 0.8280000000000015\n",
      "Episode 440, Average Evaluation Reward: 0.8280000000000015\n",
      "Average Evaluation reward over 10 episodes: -0.27600000000000025\n",
      "Episode 450, Average Evaluation Reward: -0.27600000000000025\n",
      "Average Evaluation reward over 10 episodes: -0.14200000000000018\n",
      "Episode 460, Average Evaluation Reward: -0.14200000000000018\n",
      "Average Evaluation reward over 10 episodes: 0.6500000000000007\n",
      "Episode 470, Average Evaluation Reward: 0.6500000000000007\n",
      "Average Evaluation reward over 10 episodes: 0.6450000000000014\n",
      "Episode 480, Average Evaluation Reward: 0.6450000000000014\n",
      "Average Evaluation reward over 10 episodes: 2.4520000000000017\n",
      "Episode 490, Average Evaluation Reward: 2.4520000000000017\n",
      "Average Evaluation reward over 10 episodes: 1.6919999999999984\n",
      "Episode 500, Average Evaluation Reward: 1.6919999999999984\n",
      "Average Evaluation reward over 10 episodes: 0.781000000000001\n",
      "Episode 510, Average Evaluation Reward: 0.781000000000001\n",
      "Average Evaluation reward over 10 episodes: 0.6600000000000001\n",
      "Episode 520, Average Evaluation Reward: 0.6600000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.1860000000000003\n",
      "Episode 530, Average Evaluation Reward: -0.1860000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.24899999999999983\n",
      "Episode 540, Average Evaluation Reward: 0.24899999999999983\n",
      "Average Evaluation reward over 10 episodes: 1.5400000000000016\n",
      "Episode 550, Average Evaluation Reward: 1.5400000000000016\n",
      "Average Evaluation reward over 10 episodes: -0.5760000000000004\n",
      "Episode 560, Average Evaluation Reward: -0.5760000000000004\n",
      "Average Evaluation reward over 10 episodes: 2.678\n",
      "Episode 570, Average Evaluation Reward: 2.678\n",
      "Average Evaluation reward over 10 episodes: -0.5220000000000002\n",
      "Episode 580, Average Evaluation Reward: -0.5220000000000002\n",
      "Average Evaluation reward over 10 episodes: 1.5300000000000011\n",
      "Episode 590, Average Evaluation Reward: 1.5300000000000011\n",
      "Average Evaluation reward over 10 episodes: 0.6100000000000005\n",
      "Episode 600, Average Evaluation Reward: 0.6100000000000005\n",
      "Average Evaluation reward over 10 episodes: 0.6620000000000004\n",
      "Episode 610, Average Evaluation Reward: 0.6620000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.4970000000000018\n",
      "Episode 620, Average Evaluation Reward: 0.4970000000000018\n",
      "Average Evaluation reward over 10 episodes: 1.375000000000001\n",
      "Episode 630, Average Evaluation Reward: 1.375000000000001\n",
      "Average Evaluation reward over 10 episodes: 1.5440000000000003\n",
      "Episode 640, Average Evaluation Reward: 1.5440000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.5430000000000005\n",
      "Episode 650, Average Evaluation Reward: -0.5430000000000005\n",
      "Average Evaluation reward over 10 episodes: -0.03100000000000035\n",
      "Episode 660, Average Evaluation Reward: -0.03100000000000035\n",
      "Average Evaluation reward over 10 episodes: -0.4770000000000002\n",
      "Episode 670, Average Evaluation Reward: -0.4770000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.5330000000000005\n",
      "Episode 680, Average Evaluation Reward: 0.5330000000000005\n",
      "Average Evaluation reward over 10 episodes: 2.4540000000000006\n",
      "Episode 690, Average Evaluation Reward: 2.4540000000000006\n",
      "Average Evaluation reward over 10 episodes: 0.5160000000000008\n",
      "Episode 700, Average Evaluation Reward: 0.5160000000000008\n",
      "Average Evaluation reward over 10 episodes: 1.6510000000000016\n",
      "Episode 710, Average Evaluation Reward: 1.6510000000000016\n",
      "Average Evaluation reward over 10 episodes: -0.5340000000000004\n",
      "Episode 720, Average Evaluation Reward: -0.5340000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.2950000000000003\n",
      "Episode 730, Average Evaluation Reward: -0.2950000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.751000000000001\n",
      "Episode 740, Average Evaluation Reward: 0.751000000000001\n",
      "Average Evaluation reward over 10 episodes: 0.5640000000000017\n",
      "Episode 750, Average Evaluation Reward: 0.5640000000000017\n",
      "Average Evaluation reward over 10 episodes: 0.6330000000000011\n",
      "Episode 760, Average Evaluation Reward: 0.6330000000000011\n",
      "Average Evaluation reward over 10 episodes: 0.6280000000000008\n",
      "Episode 770, Average Evaluation Reward: 0.6280000000000008\n",
      "Average Evaluation reward over 10 episodes: 0.668\n",
      "Episode 780, Average Evaluation Reward: 0.668\n",
      "Average Evaluation reward over 10 episodes: 0.7760000000000001\n",
      "Episode 790, Average Evaluation Reward: 0.7760000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.20800000000000035\n",
      "Episode 800, Average Evaluation Reward: -0.20800000000000035\n",
      "Average Evaluation reward over 10 episodes: 1.681000000000002\n",
      "Episode 810, Average Evaluation Reward: 1.681000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.19900000000000034\n",
      "Episode 820, Average Evaluation Reward: -0.19900000000000034\n",
      "Average Evaluation reward over 10 episodes: -0.36200000000000027\n",
      "Episode 830, Average Evaluation Reward: -0.36200000000000027\n",
      "Average Evaluation reward over 10 episodes: 0.7170000000000001\n",
      "Episode 840, Average Evaluation Reward: 0.7170000000000001\n",
      "Average Evaluation reward over 10 episodes: 1.553\n",
      "Episode 850, Average Evaluation Reward: 1.553\n",
      "Average Evaluation reward over 10 episodes: 0.4730000000000009\n",
      "Episode 860, Average Evaluation Reward: 0.4730000000000009\n",
      "Average Evaluation reward over 10 episodes: 0.5790000000000007\n",
      "Episode 870, Average Evaluation Reward: 0.5790000000000007\n",
      "Average Evaluation reward over 10 episodes: 2.676000000000004\n",
      "Episode 880, Average Evaluation Reward: 2.676000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.6520000000000004\n",
      "Episode 890, Average Evaluation Reward: -0.6520000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.18600000000000047\n",
      "Episode 900, Average Evaluation Reward: 0.18600000000000047\n",
      "Average Evaluation reward over 10 episodes: -0.2320000000000003\n",
      "Episode 910, Average Evaluation Reward: -0.2320000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.5150000000000003\n",
      "Episode 920, Average Evaluation Reward: 0.5150000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.29500000000000026\n",
      "Episode 930, Average Evaluation Reward: -0.29500000000000026\n",
      "Average Evaluation reward over 10 episodes: -0.38700000000000023\n",
      "Episode 940, Average Evaluation Reward: -0.38700000000000023\n",
      "Average Evaluation reward over 10 episodes: 2.550999999999993\n",
      "Episode 950, Average Evaluation Reward: 2.550999999999993\n",
      "Average Evaluation reward over 10 episodes: -0.5910000000000004\n",
      "Episode 960, Average Evaluation Reward: -0.5910000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.42800000000000027\n",
      "Episode 970, Average Evaluation Reward: -0.42800000000000027\n",
      "Average Evaluation reward over 10 episodes: 0.5630000000000007\n",
      "Episode 980, Average Evaluation Reward: 0.5630000000000007\n",
      "Average Evaluation reward over 10 episodes: -0.36700000000000027\n",
      "Episode 990, Average Evaluation Reward: -0.36700000000000027\n",
      "Average Evaluation reward over 10 episodes: -0.5000000000000002\n",
      "Episode 1000, Average Evaluation Reward: -0.5000000000000002\n",
      "Average Evaluation reward over 10 episodes: 2.7460000000000044\n",
      "Episode 1010, Average Evaluation Reward: 2.7460000000000044\n",
      "Average Evaluation reward over 10 episodes: 0.5260000000000009\n",
      "Episode 1020, Average Evaluation Reward: 0.5260000000000009\n",
      "Average Evaluation reward over 10 episodes: -0.5100000000000002\n",
      "Episode 1030, Average Evaluation Reward: -0.5100000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.2529999999999998\n",
      "Episode 1040, Average Evaluation Reward: 0.2529999999999998\n",
      "Average Evaluation reward over 10 episodes: 0.6510000000000007\n",
      "Episode 1050, Average Evaluation Reward: 0.6510000000000007\n",
      "Average Evaluation reward over 10 episodes: 1.4980000000000016\n",
      "Episode 1060, Average Evaluation Reward: 1.4980000000000016\n",
      "Average Evaluation reward over 10 episodes: 3.838000000000001\n",
      "Episode 1070, Average Evaluation Reward: 3.838000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.21500000000000022\n",
      "Episode 1080, Average Evaluation Reward: -0.21500000000000022\n",
      "Average Evaluation reward over 10 episodes: 0.683000000000001\n",
      "Episode 1090, Average Evaluation Reward: 0.683000000000001\n",
      "Average Evaluation reward over 10 episodes: 0.4930000000000005\n",
      "Episode 1100, Average Evaluation Reward: 0.4930000000000005\n",
      "Average Evaluation reward over 10 episodes: -0.7140000000000003\n",
      "Episode 1110, Average Evaluation Reward: -0.7140000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.5140000000000002\n",
      "Episode 1120, Average Evaluation Reward: -0.5140000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.36100000000000015\n",
      "Episode 1130, Average Evaluation Reward: -0.36100000000000015\n",
      "Average Evaluation reward over 10 episodes: 0.5880000000000004\n",
      "Episode 1140, Average Evaluation Reward: 0.5880000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.3610000000000006\n",
      "Episode 1150, Average Evaluation Reward: 0.3610000000000006\n",
      "Average Evaluation reward over 10 episodes: 0.7709999999999999\n",
      "Episode 1160, Average Evaluation Reward: 0.7709999999999999\n",
      "Average Evaluation reward over 10 episodes: -0.38600000000000023\n",
      "Episode 1170, Average Evaluation Reward: -0.38600000000000023\n",
      "Average Evaluation reward over 10 episodes: -0.6090000000000002\n",
      "Episode 1180, Average Evaluation Reward: -0.6090000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.8780000000000003\n",
      "Episode 1190, Average Evaluation Reward: 0.8780000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.4770000000000004\n",
      "Episode 1200, Average Evaluation Reward: 0.4770000000000004\n",
      "Average Evaluation reward over 10 episodes: 1.4920000000000013\n",
      "Episode 1210, Average Evaluation Reward: 1.4920000000000013\n",
      "Average Evaluation reward over 10 episodes: 1.6200000000000023\n",
      "Episode 1220, Average Evaluation Reward: 1.6200000000000023\n",
      "Average Evaluation reward over 10 episodes: -0.3070000000000002\n",
      "Episode 1230, Average Evaluation Reward: -0.3070000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.4270000000000003\n",
      "Episode 1240, Average Evaluation Reward: -0.4270000000000003\n",
      "Average Evaluation reward over 10 episodes: 3.030000000000002\n",
      "Episode 1250, Average Evaluation Reward: 3.030000000000002\n",
      "Average Evaluation reward over 10 episodes: 1.6630000000000007\n",
      "Episode 1260, Average Evaluation Reward: 1.6630000000000007\n",
      "Average Evaluation reward over 10 episodes: -0.36700000000000027\n",
      "Episode 1270, Average Evaluation Reward: -0.36700000000000027\n",
      "Average Evaluation reward over 10 episodes: 0.8840000000000006\n",
      "Episode 1280, Average Evaluation Reward: 0.8840000000000006\n",
      "Average Evaluation reward over 10 episodes: -0.04800000000000022\n",
      "Episode 1290, Average Evaluation Reward: -0.04800000000000022\n",
      "Average Evaluation reward over 10 episodes: -0.20400000000000018\n",
      "Episode 1300, Average Evaluation Reward: -0.20400000000000018\n",
      "Average Evaluation reward over 10 episodes: 2.6690000000000045\n",
      "Episode 1310, Average Evaluation Reward: 2.6690000000000045\n",
      "Average Evaluation reward over 10 episodes: -0.3640000000000002\n",
      "Episode 1320, Average Evaluation Reward: -0.3640000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.4120000000000005\n",
      "Episode 1330, Average Evaluation Reward: -0.4120000000000005\n",
      "Average Evaluation reward over 10 episodes: 0.5740000000000005\n",
      "Episode 1340, Average Evaluation Reward: 0.5740000000000005\n",
      "Average Evaluation reward over 10 episodes: 2.3319999999999936\n",
      "Episode 1350, Average Evaluation Reward: 2.3319999999999936\n",
      "Average Evaluation reward over 10 episodes: -0.34800000000000036\n",
      "Episode 1360, Average Evaluation Reward: -0.34800000000000036\n",
      "Average Evaluation reward over 10 episodes: -0.5160000000000003\n",
      "Episode 1370, Average Evaluation Reward: -0.5160000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.8699999999999998\n",
      "Episode 1380, Average Evaluation Reward: 0.8699999999999998\n",
      "Average Evaluation reward over 10 episodes: 0.8260000000000007\n",
      "Episode 1390, Average Evaluation Reward: 0.8260000000000007\n",
      "Average Evaluation reward over 10 episodes: 2.6599999999999984\n",
      "Episode 1400, Average Evaluation Reward: 2.6599999999999984\n",
      "Average Evaluation reward over 10 episodes: 0.40400000000000025\n",
      "Episode 1410, Average Evaluation Reward: 0.40400000000000025\n",
      "Average Evaluation reward over 10 episodes: -0.17300000000000032\n",
      "Episode 1420, Average Evaluation Reward: -0.17300000000000032\n",
      "Average Evaluation reward over 10 episodes: -0.6050000000000004\n",
      "Episode 1430, Average Evaluation Reward: -0.6050000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.548000000000001\n",
      "Episode 1440, Average Evaluation Reward: 0.548000000000001\n",
      "Average Evaluation reward over 10 episodes: 0.31300000000000006\n",
      "Episode 1450, Average Evaluation Reward: 0.31300000000000006\n",
      "Average Evaluation reward over 10 episodes: 0.5740000000000027\n",
      "Episode 1460, Average Evaluation Reward: 0.5740000000000027\n",
      "Average Evaluation reward over 10 episodes: 0.6850000000000022\n",
      "Episode 1470, Average Evaluation Reward: 0.6850000000000022\n",
      "Average Evaluation reward over 10 episodes: 0.4669999999999998\n",
      "Episode 1480, Average Evaluation Reward: 0.4669999999999998\n",
      "Average Evaluation reward over 10 episodes: -0.4530000000000002\n",
      "Episode 1490, Average Evaluation Reward: -0.4530000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.26300000000000023\n",
      "Episode 1500, Average Evaluation Reward: -0.26300000000000023\n",
      "Average Evaluation reward over 10 episodes: 0.9890000000000008\n",
      "Episode 1510, Average Evaluation Reward: 0.9890000000000008\n",
      "Average Evaluation reward over 10 episodes: 0.5720000000000004\n",
      "Episode 1520, Average Evaluation Reward: 0.5720000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.11400000000000014\n",
      "Episode 1530, Average Evaluation Reward: -0.11400000000000014\n",
      "Average Evaluation reward over 10 episodes: -0.32400000000000023\n",
      "Episode 1540, Average Evaluation Reward: -0.32400000000000023\n",
      "Average Evaluation reward over 10 episodes: -0.24100000000000016\n",
      "Episode 1550, Average Evaluation Reward: -0.24100000000000016\n",
      "Average Evaluation reward over 10 episodes: -0.21900000000000022\n",
      "Episode 1560, Average Evaluation Reward: -0.21900000000000022\n",
      "Average Evaluation reward over 10 episodes: 1.2820000000000005\n",
      "Episode 1570, Average Evaluation Reward: 1.2820000000000005\n",
      "Average Evaluation reward over 10 episodes: -0.3780000000000002\n",
      "Episode 1580, Average Evaluation Reward: -0.3780000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.34200000000000036\n",
      "Episode 1590, Average Evaluation Reward: -0.34200000000000036\n",
      "Average Evaluation reward over 10 episodes: -0.07000000000000024\n",
      "Episode 1600, Average Evaluation Reward: -0.07000000000000024\n",
      "Average Evaluation reward over 10 episodes: 1.516000000000002\n",
      "Episode 1610, Average Evaluation Reward: 1.516000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.20400000000000018\n",
      "Episode 1620, Average Evaluation Reward: -0.20400000000000018\n",
      "Average Evaluation reward over 10 episodes: -0.40800000000000053\n",
      "Episode 1630, Average Evaluation Reward: -0.40800000000000053\n",
      "Average Evaluation reward over 10 episodes: -0.3300000000000002\n",
      "Episode 1640, Average Evaluation Reward: -0.3300000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.5160000000000002\n",
      "Episode 1650, Average Evaluation Reward: -0.5160000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.7120000000000002\n",
      "Episode 1660, Average Evaluation Reward: 0.7120000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.6730000000000006\n",
      "Episode 1670, Average Evaluation Reward: 0.6730000000000006\n",
      "Average Evaluation reward over 10 episodes: -0.3080000000000002\n",
      "Episode 1680, Average Evaluation Reward: -0.3080000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.14700000000000024\n",
      "Episode 1690, Average Evaluation Reward: -0.14700000000000024\n",
      "Average Evaluation reward over 10 episodes: 0.43400000000000016\n",
      "Episode 1700, Average Evaluation Reward: 0.43400000000000016\n",
      "Average Evaluation reward over 10 episodes: 1.8190000000000004\n",
      "Episode 1710, Average Evaluation Reward: 1.8190000000000004\n",
      "Average Evaluation reward over 10 episodes: 2.4930000000000025\n",
      "Episode 1720, Average Evaluation Reward: 2.4930000000000025\n",
      "Average Evaluation reward over 10 episodes: 0.36400000000000043\n",
      "Episode 1730, Average Evaluation Reward: 0.36400000000000043\n",
      "Average Evaluation reward over 10 episodes: -0.2250000000000001\n",
      "Episode 1740, Average Evaluation Reward: -0.2250000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.4640000000000003\n",
      "Episode 1750, Average Evaluation Reward: -0.4640000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.4750000000000002\n",
      "Episode 1760, Average Evaluation Reward: -0.4750000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.3390000000000003\n",
      "Episode 1770, Average Evaluation Reward: -0.3390000000000003\n",
      "Average Evaluation reward over 10 episodes: 1.7530000000000006\n",
      "Episode 1780, Average Evaluation Reward: 1.7530000000000006\n",
      "Average Evaluation reward over 10 episodes: -0.3750000000000003\n",
      "Episode 1790, Average Evaluation Reward: -0.3750000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.49100000000000027\n",
      "Episode 1800, Average Evaluation Reward: 0.49100000000000027\n",
      "Average Evaluation reward over 10 episodes: -0.2400000000000003\n",
      "Episode 1810, Average Evaluation Reward: -0.2400000000000003\n",
      "Average Evaluation reward over 10 episodes: 2.633\n",
      "Episode 1820, Average Evaluation Reward: 2.633\n",
      "Average Evaluation reward over 10 episodes: -0.34000000000000025\n",
      "Episode 1830, Average Evaluation Reward: -0.34000000000000025\n",
      "Average Evaluation reward over 10 episodes: -0.5770000000000004\n",
      "Episode 1840, Average Evaluation Reward: -0.5770000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.7390000000000002\n",
      "Episode 1850, Average Evaluation Reward: 0.7390000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.7059999999999997\n",
      "Episode 1860, Average Evaluation Reward: 0.7059999999999997\n",
      "Average Evaluation reward over 10 episodes: -0.4780000000000003\n",
      "Episode 1870, Average Evaluation Reward: -0.4780000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.31000000000000016\n",
      "Episode 1880, Average Evaluation Reward: -0.31000000000000016\n",
      "Average Evaluation reward over 10 episodes: 0.6670000000000005\n",
      "Episode 1890, Average Evaluation Reward: 0.6670000000000005\n",
      "Average Evaluation reward over 10 episodes: 0.7240000000000004\n",
      "Episode 1900, Average Evaluation Reward: 0.7240000000000004\n",
      "Average Evaluation reward over 10 episodes: 1.7280000000000009\n",
      "Episode 1910, Average Evaluation Reward: 1.7280000000000009\n",
      "Average Evaluation reward over 10 episodes: -0.20000000000000018\n",
      "Episode 1920, Average Evaluation Reward: -0.20000000000000018\n",
      "Average Evaluation reward over 10 episodes: -0.3090000000000003\n",
      "Episode 1930, Average Evaluation Reward: -0.3090000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.17700000000000024\n",
      "Episode 1940, Average Evaluation Reward: -0.17700000000000024\n",
      "Average Evaluation reward over 10 episodes: 2.684000000000002\n",
      "Episode 1950, Average Evaluation Reward: 2.684000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.5570000000000003\n",
      "Episode 1960, Average Evaluation Reward: -0.5570000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.5220000000000005\n",
      "Episode 1970, Average Evaluation Reward: -0.5220000000000005\n",
      "Average Evaluation reward over 10 episodes: -0.5640000000000003\n",
      "Episode 1980, Average Evaluation Reward: -0.5640000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.5670000000000003\n",
      "Episode 1990, Average Evaluation Reward: -0.5670000000000003\n",
      "Average Evaluation reward over 10 episodes: 1.5289999999999957\n",
      "Episode 2000, Average Evaluation Reward: 1.5289999999999957\n",
      "Average Evaluation reward over 10 episodes: -0.07100000000000026\n",
      "Episode 2010, Average Evaluation Reward: -0.07100000000000026\n",
      "Average Evaluation reward over 10 episodes: 0.4659999999999999\n",
      "Episode 2020, Average Evaluation Reward: 0.4659999999999999\n",
      "Average Evaluation reward over 10 episodes: -0.3160000000000002\n",
      "Episode 2030, Average Evaluation Reward: -0.3160000000000002\n",
      "Average Evaluation reward over 10 episodes: 1.4070000000000034\n",
      "Episode 2040, Average Evaluation Reward: 1.4070000000000034\n",
      "Average Evaluation reward over 10 episodes: 1.6420000000000037\n",
      "Episode 2050, Average Evaluation Reward: 1.6420000000000037\n",
      "Average Evaluation reward over 10 episodes: -0.2360000000000003\n",
      "Episode 2060, Average Evaluation Reward: -0.2360000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.6360000000000016\n",
      "Episode 2070, Average Evaluation Reward: 0.6360000000000016\n",
      "Average Evaluation reward over 10 episodes: 0.5160000000000003\n",
      "Episode 2080, Average Evaluation Reward: 0.5160000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.7910000000000006\n",
      "Episode 2090, Average Evaluation Reward: -0.7910000000000006\n",
      "Average Evaluation reward over 10 episodes: -0.6120000000000003\n",
      "Episode 2100, Average Evaluation Reward: -0.6120000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.7730000000000005\n",
      "Episode 2110, Average Evaluation Reward: -0.7730000000000005\n",
      "Average Evaluation reward over 10 episodes: 0.5050000000000013\n",
      "Episode 2120, Average Evaluation Reward: 0.5050000000000013\n",
      "Average Evaluation reward over 10 episodes: -0.4490000000000002\n",
      "Episode 2130, Average Evaluation Reward: -0.4490000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.49300000000000094\n",
      "Episode 2140, Average Evaluation Reward: 0.49300000000000094\n",
      "Average Evaluation reward over 10 episodes: 1.3630000000000002\n",
      "Episode 2150, Average Evaluation Reward: 1.3630000000000002\n",
      "Average Evaluation reward over 10 episodes: 1.7520000000000011\n",
      "Episode 2160, Average Evaluation Reward: 1.7520000000000011\n",
      "Average Evaluation reward over 10 episodes: -0.44700000000000034\n",
      "Episode 2170, Average Evaluation Reward: -0.44700000000000034\n",
      "Average Evaluation reward over 10 episodes: -0.40700000000000014\n",
      "Episode 2180, Average Evaluation Reward: -0.40700000000000014\n",
      "Average Evaluation reward over 10 episodes: -0.5290000000000004\n",
      "Episode 2190, Average Evaluation Reward: -0.5290000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.5680000000000015\n",
      "Episode 2200, Average Evaluation Reward: 0.5680000000000015\n",
      "Average Evaluation reward over 10 episodes: -0.4630000000000002\n",
      "Episode 2210, Average Evaluation Reward: -0.4630000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.44800000000000023\n",
      "Episode 2220, Average Evaluation Reward: -0.44800000000000023\n",
      "Average Evaluation reward over 10 episodes: 0.7010000000000007\n",
      "Episode 2230, Average Evaluation Reward: 0.7010000000000007\n",
      "Average Evaluation reward over 10 episodes: 0.5870000000000004\n",
      "Episode 2240, Average Evaluation Reward: 0.5870000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.5969999999999999\n",
      "Episode 2250, Average Evaluation Reward: 0.5969999999999999\n",
      "Average Evaluation reward over 10 episodes: 1.8579999999999983\n",
      "Episode 2260, Average Evaluation Reward: 1.8579999999999983\n",
      "Average Evaluation reward over 10 episodes: 0.4340000000000009\n",
      "Episode 2270, Average Evaluation Reward: 0.4340000000000009\n",
      "Average Evaluation reward over 10 episodes: 0.7040000000000005\n",
      "Episode 2280, Average Evaluation Reward: 0.7040000000000005\n",
      "Average Evaluation reward over 10 episodes: 0.66\n",
      "Episode 2290, Average Evaluation Reward: 0.66\n",
      "Average Evaluation reward over 10 episodes: 0.7290000000000008\n",
      "Episode 2300, Average Evaluation Reward: 0.7290000000000008\n",
      "Average Evaluation reward over 10 episodes: 0.6990000000000005\n",
      "Episode 2310, Average Evaluation Reward: 0.6990000000000005\n",
      "Average Evaluation reward over 10 episodes: -0.40500000000000025\n",
      "Episode 2320, Average Evaluation Reward: -0.40500000000000025\n",
      "Average Evaluation reward over 10 episodes: -0.4270000000000002\n",
      "Episode 2330, Average Evaluation Reward: -0.4270000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.5440000000000003\n",
      "Episode 2340, Average Evaluation Reward: -0.5440000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.11900000000000036\n",
      "Episode 2350, Average Evaluation Reward: 0.11900000000000036\n",
      "Average Evaluation reward over 10 episodes: 2.581000000000004\n",
      "Episode 2360, Average Evaluation Reward: 2.581000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.4360000000000001\n",
      "Episode 2370, Average Evaluation Reward: 0.4360000000000001\n",
      "Average Evaluation reward over 10 episodes: 0.35299999999999987\n",
      "Episode 2380, Average Evaluation Reward: 0.35299999999999987\n",
      "Average Evaluation reward over 10 episodes: 0.5150000000000008\n",
      "Episode 2390, Average Evaluation Reward: 0.5150000000000008\n",
      "Average Evaluation reward over 10 episodes: 0.5480000000000002\n",
      "Episode 2400, Average Evaluation Reward: 0.5480000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.6310000000000004\n",
      "Episode 2410, Average Evaluation Reward: -0.6310000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.7710000000000004\n",
      "Episode 2420, Average Evaluation Reward: -0.7710000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.36200000000000015\n",
      "Episode 2430, Average Evaluation Reward: -0.36200000000000015\n",
      "Average Evaluation reward over 10 episodes: -0.3530000000000003\n",
      "Episode 2440, Average Evaluation Reward: -0.3530000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.6100000000000004\n",
      "Episode 2450, Average Evaluation Reward: -0.6100000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.4370000000000004\n",
      "Episode 2460, Average Evaluation Reward: 0.4370000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.5720000000000003\n",
      "Episode 2470, Average Evaluation Reward: -0.5720000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.48300000000000026\n",
      "Episode 2480, Average Evaluation Reward: -0.48300000000000026\n",
      "Average Evaluation reward over 10 episodes: 0.47000000000000225\n",
      "Episode 2490, Average Evaluation Reward: 0.47000000000000225\n",
      "Average Evaluation reward over 10 episodes: 0.4990000000000002\n",
      "Episode 2500, Average Evaluation Reward: 0.4990000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.6900000000000002\n",
      "Episode 2510, Average Evaluation Reward: 0.6900000000000002\n",
      "Average Evaluation reward over 10 episodes: 1.2280000000000015\n",
      "Episode 2520, Average Evaluation Reward: 1.2280000000000015\n",
      "Average Evaluation reward over 10 episodes: 0.40000000000000113\n",
      "Episode 2530, Average Evaluation Reward: 0.40000000000000113\n",
      "Average Evaluation reward over 10 episodes: -0.49200000000000016\n",
      "Episode 2540, Average Evaluation Reward: -0.49200000000000016\n",
      "Average Evaluation reward over 10 episodes: -0.3690000000000001\n",
      "Episode 2550, Average Evaluation Reward: -0.3690000000000001\n",
      "Average Evaluation reward over 10 episodes: 2.5550000000000015\n",
      "Episode 2560, Average Evaluation Reward: 2.5550000000000015\n",
      "Average Evaluation reward over 10 episodes: -0.6650000000000004\n",
      "Episode 2570, Average Evaluation Reward: -0.6650000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.36700000000000016\n",
      "Episode 2580, Average Evaluation Reward: -0.36700000000000016\n",
      "Average Evaluation reward over 10 episodes: -0.6370000000000002\n",
      "Episode 2590, Average Evaluation Reward: -0.6370000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.2730000000000002\n",
      "Episode 2600, Average Evaluation Reward: -0.2730000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.4690000000000003\n",
      "Episode 2610, Average Evaluation Reward: -0.4690000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.5240000000000002\n",
      "Episode 2620, Average Evaluation Reward: 0.5240000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.4280000000000001\n",
      "Episode 2630, Average Evaluation Reward: -0.4280000000000001\n",
      "Average Evaluation reward over 10 episodes: 0.332\n",
      "Episode 2640, Average Evaluation Reward: 0.332\n",
      "Average Evaluation reward over 10 episodes: -0.4340000000000003\n",
      "Episode 2650, Average Evaluation Reward: -0.4340000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.29700000000000015\n",
      "Episode 2660, Average Evaluation Reward: -0.29700000000000015\n",
      "Average Evaluation reward over 10 episodes: 0.8030000000000002\n",
      "Episode 2670, Average Evaluation Reward: 0.8030000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.41600000000000054\n",
      "Episode 2680, Average Evaluation Reward: 0.41600000000000054\n",
      "Average Evaluation reward over 10 episodes: 1.380000000000004\n",
      "Episode 2690, Average Evaluation Reward: 1.380000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.8020000000000002\n",
      "Episode 2700, Average Evaluation Reward: 0.8020000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.5160000000000002\n",
      "Episode 2710, Average Evaluation Reward: -0.5160000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.38300000000000034\n",
      "Episode 2720, Average Evaluation Reward: -0.38300000000000034\n",
      "Average Evaluation reward over 10 episodes: -0.6920000000000005\n",
      "Episode 2730, Average Evaluation Reward: -0.6920000000000005\n",
      "Average Evaluation reward over 10 episodes: -0.28300000000000003\n",
      "Episode 2740, Average Evaluation Reward: -0.28300000000000003\n",
      "Average Evaluation reward over 10 episodes: 1.5310000000000006\n",
      "Episode 2750, Average Evaluation Reward: 1.5310000000000006\n",
      "Average Evaluation reward over 10 episodes: -0.4380000000000003\n",
      "Episode 2760, Average Evaluation Reward: -0.4380000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.4610000000000002\n",
      "Episode 2770, Average Evaluation Reward: -0.4610000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.8580000000000002\n",
      "Episode 2780, Average Evaluation Reward: 0.8580000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.5360000000000003\n",
      "Episode 2790, Average Evaluation Reward: -0.5360000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.5600000000000003\n",
      "Episode 2800, Average Evaluation Reward: -0.5600000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.29100000000000026\n",
      "Episode 2810, Average Evaluation Reward: -0.29100000000000026\n",
      "Average Evaluation reward over 10 episodes: -0.30500000000000016\n",
      "Episode 2820, Average Evaluation Reward: -0.30500000000000016\n",
      "Average Evaluation reward over 10 episodes: -0.3460000000000003\n",
      "Episode 2830, Average Evaluation Reward: -0.3460000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.9270000000000003\n",
      "Episode 2840, Average Evaluation Reward: 0.9270000000000003\n",
      "Average Evaluation reward over 10 episodes: 1.4110000000000011\n",
      "Episode 2850, Average Evaluation Reward: 1.4110000000000011\n",
      "Average Evaluation reward over 10 episodes: 0.4640000000000006\n",
      "Episode 2860, Average Evaluation Reward: 0.4640000000000006\n",
      "Average Evaluation reward over 10 episodes: -0.4980000000000002\n",
      "Episode 2870, Average Evaluation Reward: -0.4980000000000002\n",
      "Average Evaluation reward over 10 episodes: 1.5600000000000027\n",
      "Episode 2880, Average Evaluation Reward: 1.5600000000000027\n",
      "Average Evaluation reward over 10 episodes: -0.5420000000000003\n",
      "Episode 2890, Average Evaluation Reward: -0.5420000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.6790000000000004\n",
      "Episode 2900, Average Evaluation Reward: -0.6790000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.3410000000000003\n",
      "Episode 2910, Average Evaluation Reward: -0.3410000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.6670000000000005\n",
      "Episode 2920, Average Evaluation Reward: -0.6670000000000005\n",
      "Average Evaluation reward over 10 episodes: -0.5440000000000003\n",
      "Episode 2930, Average Evaluation Reward: -0.5440000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.45300000000000074\n",
      "Episode 2940, Average Evaluation Reward: 0.45300000000000074\n",
      "Average Evaluation reward over 10 episodes: 0.3270000000000003\n",
      "Episode 2950, Average Evaluation Reward: 0.3270000000000003\n",
      "Average Evaluation reward over 10 episodes: 1.5270000000000006\n",
      "Episode 2960, Average Evaluation Reward: 1.5270000000000006\n",
      "Average Evaluation reward over 10 episodes: 1.446000000000002\n",
      "Episode 2970, Average Evaluation Reward: 1.446000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.5120000000000002\n",
      "Episode 2980, Average Evaluation Reward: -0.5120000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.6190000000000002\n",
      "Episode 2990, Average Evaluation Reward: 0.6190000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.37300000000000083\n",
      "Episode 3000, Average Evaluation Reward: 0.37300000000000083\n",
      "Average Evaluation reward over 10 episodes: -0.5380000000000004\n",
      "Episode 3010, Average Evaluation Reward: -0.5380000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.36500000000000127\n",
      "Episode 3020, Average Evaluation Reward: 0.36500000000000127\n",
      "Average Evaluation reward over 10 episodes: 2.4750000000000023\n",
      "Episode 3030, Average Evaluation Reward: 2.4750000000000023\n",
      "Average Evaluation reward over 10 episodes: -0.5120000000000002\n",
      "Episode 3040, Average Evaluation Reward: -0.5120000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.5730000000000015\n",
      "Episode 3050, Average Evaluation Reward: 0.5730000000000015\n",
      "Average Evaluation reward over 10 episodes: 0.5580000000000006\n",
      "Episode 3060, Average Evaluation Reward: 0.5580000000000006\n",
      "Average Evaluation reward over 10 episodes: -0.6290000000000004\n",
      "Episode 3070, Average Evaluation Reward: -0.6290000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.5230000000000004\n",
      "Episode 3080, Average Evaluation Reward: -0.5230000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.7910000000000004\n",
      "Episode 3090, Average Evaluation Reward: -0.7910000000000004\n",
      "Average Evaluation reward over 10 episodes: 4.574000000000001\n",
      "Episode 3100, Average Evaluation Reward: 4.574000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.39400000000000024\n",
      "Episode 3110, Average Evaluation Reward: -0.39400000000000024\n",
      "Average Evaluation reward over 10 episodes: 0.3790000000000009\n",
      "Episode 3120, Average Evaluation Reward: 0.3790000000000009\n",
      "Average Evaluation reward over 10 episodes: 1.4030000000000016\n",
      "Episode 3130, Average Evaluation Reward: 1.4030000000000016\n",
      "Average Evaluation reward over 10 episodes: -0.44800000000000023\n",
      "Episode 3140, Average Evaluation Reward: -0.44800000000000023\n",
      "Average Evaluation reward over 10 episodes: 1.5940000000000007\n",
      "Episode 3150, Average Evaluation Reward: 1.5940000000000007\n",
      "Average Evaluation reward over 10 episodes: 1.684000000000001\n",
      "Episode 3160, Average Evaluation Reward: 1.684000000000001\n",
      "Average Evaluation reward over 10 episodes: 0.5720000000000003\n",
      "Episode 3170, Average Evaluation Reward: 0.5720000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.4410000000000003\n",
      "Episode 3180, Average Evaluation Reward: -0.4410000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.908\n",
      "Episode 3190, Average Evaluation Reward: 0.908\n",
      "Average Evaluation reward over 10 episodes: -0.6170000000000002\n",
      "Episode 3200, Average Evaluation Reward: -0.6170000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.4410000000000007\n",
      "Episode 3210, Average Evaluation Reward: 0.4410000000000007\n",
      "Average Evaluation reward over 10 episodes: -0.3360000000000002\n",
      "Episode 3220, Average Evaluation Reward: -0.3360000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.7580000000000003\n",
      "Episode 3230, Average Evaluation Reward: 0.7580000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.4230000000000003\n",
      "Episode 3240, Average Evaluation Reward: -0.4230000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.4150000000000003\n",
      "Episode 3250, Average Evaluation Reward: -0.4150000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.3620000000000002\n",
      "Episode 3260, Average Evaluation Reward: -0.3620000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.3090000000000003\n",
      "Episode 3270, Average Evaluation Reward: -0.3090000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.7310000000000001\n",
      "Episode 3280, Average Evaluation Reward: 0.7310000000000001\n",
      "Average Evaluation reward over 10 episodes: 1.4819999999999993\n",
      "Episode 3290, Average Evaluation Reward: 1.4819999999999993\n",
      "Average Evaluation reward over 10 episodes: 0.728\n",
      "Episode 3300, Average Evaluation Reward: 0.728\n",
      "Average Evaluation reward over 10 episodes: -0.33900000000000025\n",
      "Episode 3310, Average Evaluation Reward: -0.33900000000000025\n",
      "Average Evaluation reward over 10 episodes: 1.4200000000000046\n",
      "Episode 3320, Average Evaluation Reward: 1.4200000000000046\n",
      "Average Evaluation reward over 10 episodes: 0.624\n",
      "Episode 3330, Average Evaluation Reward: 0.624\n",
      "Average Evaluation reward over 10 episodes: 0.73\n",
      "Episode 3340, Average Evaluation Reward: 0.73\n",
      "Average Evaluation reward over 10 episodes: -0.5420000000000001\n",
      "Episode 3350, Average Evaluation Reward: -0.5420000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.25600000000000017\n",
      "Episode 3360, Average Evaluation Reward: -0.25600000000000017\n",
      "Average Evaluation reward over 10 episodes: 3.716\n",
      "Episode 3370, Average Evaluation Reward: 3.716\n",
      "Average Evaluation reward over 10 episodes: -0.49100000000000027\n",
      "Episode 3380, Average Evaluation Reward: -0.49100000000000027\n",
      "Average Evaluation reward over 10 episodes: -0.5080000000000002\n",
      "Episode 3390, Average Evaluation Reward: -0.5080000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.3480000000000002\n",
      "Episode 3400, Average Evaluation Reward: -0.3480000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.5640000000000003\n",
      "Episode 3410, Average Evaluation Reward: -0.5640000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.45600000000000024\n",
      "Episode 3420, Average Evaluation Reward: -0.45600000000000024\n",
      "Average Evaluation reward over 10 episodes: -0.5930000000000004\n",
      "Episode 3430, Average Evaluation Reward: -0.5930000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.5500000000000003\n",
      "Episode 3440, Average Evaluation Reward: -0.5500000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.35200000000000176\n",
      "Episode 3450, Average Evaluation Reward: 0.35200000000000176\n",
      "Average Evaluation reward over 10 episodes: 0.4459999999999999\n",
      "Episode 3460, Average Evaluation Reward: 0.4459999999999999\n",
      "Average Evaluation reward over 10 episodes: -0.46300000000000024\n",
      "Episode 3470, Average Evaluation Reward: -0.46300000000000024\n",
      "Average Evaluation reward over 10 episodes: -0.5960000000000003\n",
      "Episode 3480, Average Evaluation Reward: -0.5960000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.8720000000000004\n",
      "Episode 3490, Average Evaluation Reward: -0.8720000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.6230000000000003\n",
      "Episode 3500, Average Evaluation Reward: -0.6230000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.34600000000000014\n",
      "Episode 3510, Average Evaluation Reward: -0.34600000000000014\n",
      "Average Evaluation reward over 10 episodes: -0.5050000000000002\n",
      "Episode 3520, Average Evaluation Reward: -0.5050000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.2900000000000001\n",
      "Episode 3530, Average Evaluation Reward: -0.2900000000000001\n",
      "Average Evaluation reward over 10 episodes: 2.5820000000000007\n",
      "Episode 3540, Average Evaluation Reward: 2.5820000000000007\n",
      "Average Evaluation reward over 10 episodes: 0.6540000000000001\n",
      "Episode 3550, Average Evaluation Reward: 0.6540000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.5100000000000002\n",
      "Episode 3560, Average Evaluation Reward: -0.5100000000000002\n",
      "Average Evaluation reward over 10 episodes: 1.657\n",
      "Episode 3570, Average Evaluation Reward: 1.657\n",
      "Average Evaluation reward over 10 episodes: -0.5470000000000004\n",
      "Episode 3580, Average Evaluation Reward: -0.5470000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.6050000000000002\n",
      "Episode 3590, Average Evaluation Reward: 0.6050000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.3340000000000001\n",
      "Episode 3600, Average Evaluation Reward: -0.3340000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.24100000000000027\n",
      "Episode 3610, Average Evaluation Reward: -0.24100000000000027\n",
      "Average Evaluation reward over 10 episodes: -0.6310000000000003\n",
      "Episode 3620, Average Evaluation Reward: -0.6310000000000003\n",
      "Average Evaluation reward over 10 episodes: 2.5170000000000012\n",
      "Episode 3630, Average Evaluation Reward: 2.5170000000000012\n",
      "Average Evaluation reward over 10 episodes: -0.2300000000000002\n",
      "Episode 3640, Average Evaluation Reward: -0.2300000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.3570000000000001\n",
      "Episode 3650, Average Evaluation Reward: -0.3570000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.21800000000000014\n",
      "Episode 3660, Average Evaluation Reward: -0.21800000000000014\n",
      "Average Evaluation reward over 10 episodes: -0.40600000000000025\n",
      "Episode 3670, Average Evaluation Reward: -0.40600000000000025\n",
      "Average Evaluation reward over 10 episodes: 0.6740000000000003\n",
      "Episode 3680, Average Evaluation Reward: 0.6740000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.5020000000000003\n",
      "Episode 3690, Average Evaluation Reward: -0.5020000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.24000000000000007\n",
      "Episode 3700, Average Evaluation Reward: -0.24000000000000007\n",
      "Average Evaluation reward over 10 episodes: -0.43300000000000016\n",
      "Episode 3710, Average Evaluation Reward: -0.43300000000000016\n",
      "Average Evaluation reward over 10 episodes: 0.6340000000000011\n",
      "Episode 3720, Average Evaluation Reward: 0.6340000000000011\n",
      "Average Evaluation reward over 10 episodes: -0.5140000000000002\n",
      "Episode 3730, Average Evaluation Reward: -0.5140000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.3780000000000002\n",
      "Episode 3740, Average Evaluation Reward: -0.3780000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.6830000000000004\n",
      "Episode 3750, Average Evaluation Reward: -0.6830000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.5340000000000004\n",
      "Episode 3760, Average Evaluation Reward: -0.5340000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.6620000000000005\n",
      "Episode 3770, Average Evaluation Reward: -0.6620000000000005\n",
      "Average Evaluation reward over 10 episodes: -0.5330000000000003\n",
      "Episode 3780, Average Evaluation Reward: -0.5330000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.21100000000000022\n",
      "Episode 3790, Average Evaluation Reward: -0.21100000000000022\n",
      "Average Evaluation reward over 10 episodes: -0.6600000000000004\n",
      "Episode 3800, Average Evaluation Reward: -0.6600000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.5330000000000003\n",
      "Episode 3810, Average Evaluation Reward: -0.5330000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.7760000000000005\n",
      "Episode 3820, Average Evaluation Reward: -0.7760000000000005\n",
      "Average Evaluation reward over 10 episodes: 0.34500000000000053\n",
      "Episode 3830, Average Evaluation Reward: 0.34500000000000053\n",
      "Average Evaluation reward over 10 episodes: -0.4700000000000003\n",
      "Episode 3840, Average Evaluation Reward: -0.4700000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.40100000000000013\n",
      "Episode 3850, Average Evaluation Reward: -0.40100000000000013\n",
      "Average Evaluation reward over 10 episodes: -0.5390000000000004\n",
      "Episode 3860, Average Evaluation Reward: -0.5390000000000004\n",
      "Average Evaluation reward over 10 episodes: 1.6390000000000016\n",
      "Episode 3870, Average Evaluation Reward: 1.6390000000000016\n",
      "Average Evaluation reward over 10 episodes: -0.44200000000000017\n",
      "Episode 3880, Average Evaluation Reward: -0.44200000000000017\n",
      "Average Evaluation reward over 10 episodes: -0.39400000000000013\n",
      "Episode 3890, Average Evaluation Reward: -0.39400000000000013\n",
      "Average Evaluation reward over 10 episodes: -0.30100000000000016\n",
      "Episode 3900, Average Evaluation Reward: -0.30100000000000016\n",
      "Average Evaluation reward over 10 episodes: -0.41900000000000015\n",
      "Episode 3910, Average Evaluation Reward: -0.41900000000000015\n",
      "Average Evaluation reward over 10 episodes: -0.38400000000000034\n",
      "Episode 3920, Average Evaluation Reward: -0.38400000000000034\n",
      "Average Evaluation reward over 10 episodes: -0.5850000000000002\n",
      "Episode 3930, Average Evaluation Reward: -0.5850000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.605\n",
      "Episode 3940, Average Evaluation Reward: 0.605\n",
      "Average Evaluation reward over 10 episodes: -0.27700000000000025\n",
      "Episode 3950, Average Evaluation Reward: -0.27700000000000025\n",
      "Average Evaluation reward over 10 episodes: 0.5180000000000009\n",
      "Episode 3960, Average Evaluation Reward: 0.5180000000000009\n",
      "Average Evaluation reward over 10 episodes: -0.4880000000000003\n",
      "Episode 3970, Average Evaluation Reward: -0.4880000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.6120000000000003\n",
      "Episode 3980, Average Evaluation Reward: -0.6120000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.4510000000000002\n",
      "Episode 3990, Average Evaluation Reward: -0.4510000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.3370000000000002\n",
      "Episode 4000, Average Evaluation Reward: -0.3370000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.31000000000000033\n",
      "Episode 4010, Average Evaluation Reward: -0.31000000000000033\n",
      "Average Evaluation reward over 10 episodes: 0.532000000000002\n",
      "Episode 4020, Average Evaluation Reward: 0.532000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.15500000000000003\n",
      "Episode 4030, Average Evaluation Reward: -0.15500000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.39800000000000013\n",
      "Episode 4040, Average Evaluation Reward: -0.39800000000000013\n",
      "Average Evaluation reward over 10 episodes: -0.34300000000000014\n",
      "Episode 4050, Average Evaluation Reward: -0.34300000000000014\n",
      "Average Evaluation reward over 10 episodes: -0.32000000000000023\n",
      "Episode 4060, Average Evaluation Reward: -0.32000000000000023\n",
      "Average Evaluation reward over 10 episodes: -0.14700000000000013\n",
      "Episode 4070, Average Evaluation Reward: -0.14700000000000013\n",
      "Average Evaluation reward over 10 episodes: -0.6120000000000004\n",
      "Episode 4080, Average Evaluation Reward: -0.6120000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.4430000000000002\n",
      "Episode 4090, Average Evaluation Reward: -0.4430000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.3270000000000003\n",
      "Episode 4100, Average Evaluation Reward: -0.3270000000000003\n",
      "Average Evaluation reward over 10 episodes: 1.7019999999999995\n",
      "Episode 4110, Average Evaluation Reward: 1.7019999999999995\n",
      "Average Evaluation reward over 10 episodes: -0.3930000000000001\n",
      "Episode 4120, Average Evaluation Reward: -0.3930000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.5240000000000004\n",
      "Episode 4130, Average Evaluation Reward: -0.5240000000000004\n",
      "Average Evaluation reward over 10 episodes: 0.8640000000000008\n",
      "Episode 4140, Average Evaluation Reward: 0.8640000000000008\n",
      "Average Evaluation reward over 10 episodes: 0.643\n",
      "Episode 4150, Average Evaluation Reward: 0.643\n",
      "Average Evaluation reward over 10 episodes: 0.672\n",
      "Episode 4160, Average Evaluation Reward: 0.672\n",
      "Average Evaluation reward over 10 episodes: -0.5450000000000002\n",
      "Episode 4170, Average Evaluation Reward: -0.5450000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.4410000000000003\n",
      "Episode 4180, Average Evaluation Reward: -0.4410000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.5370000000000003\n",
      "Episode 4190, Average Evaluation Reward: -0.5370000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.2410000000000002\n",
      "Episode 4200, Average Evaluation Reward: -0.2410000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.5580000000000005\n",
      "Episode 4210, Average Evaluation Reward: 0.5580000000000005\n",
      "Average Evaluation reward over 10 episodes: -0.22300000000000014\n",
      "Episode 4220, Average Evaluation Reward: -0.22300000000000014\n",
      "Average Evaluation reward over 10 episodes: -0.7040000000000004\n",
      "Episode 4230, Average Evaluation Reward: -0.7040000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.1910000000000002\n",
      "Episode 4240, Average Evaluation Reward: -0.1910000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.48000000000000026\n",
      "Episode 4250, Average Evaluation Reward: -0.48000000000000026\n",
      "Average Evaluation reward over 10 episodes: -0.44700000000000023\n",
      "Episode 4260, Average Evaluation Reward: -0.44700000000000023\n",
      "Average Evaluation reward over 10 episodes: 0.36400000000000077\n",
      "Episode 4270, Average Evaluation Reward: 0.36400000000000077\n",
      "Average Evaluation reward over 10 episodes: -0.4370000000000003\n",
      "Episode 4280, Average Evaluation Reward: -0.4370000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.5330000000000013\n",
      "Episode 4290, Average Evaluation Reward: 0.5330000000000013\n",
      "Average Evaluation reward over 10 episodes: -0.46700000000000025\n",
      "Episode 4300, Average Evaluation Reward: -0.46700000000000025\n",
      "Average Evaluation reward over 10 episodes: 0.5440000000000002\n",
      "Episode 4310, Average Evaluation Reward: 0.5440000000000002\n",
      "Average Evaluation reward over 10 episodes: 0.4740000000000005\n",
      "Episode 4320, Average Evaluation Reward: 0.4740000000000005\n",
      "Average Evaluation reward over 10 episodes: 1.9090000000000003\n",
      "Episode 4330, Average Evaluation Reward: 1.9090000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.3840000000000001\n",
      "Episode 4340, Average Evaluation Reward: -0.3840000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.3880000000000002\n",
      "Episode 4350, Average Evaluation Reward: -0.3880000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.2860000000000003\n",
      "Episode 4360, Average Evaluation Reward: -0.2860000000000003\n",
      "Average Evaluation reward over 10 episodes: -0.4520000000000001\n",
      "Episode 4370, Average Evaluation Reward: -0.4520000000000001\n",
      "Average Evaluation reward over 10 episodes: -0.4320000000000003\n",
      "Episode 4380, Average Evaluation Reward: -0.4320000000000003\n",
      "Average Evaluation reward over 10 episodes: 0.37399999999999994\n",
      "Episode 4390, Average Evaluation Reward: 0.37399999999999994\n",
      "Average Evaluation reward over 10 episodes: 0.6460000000000009\n",
      "Episode 4400, Average Evaluation Reward: 0.6460000000000009\n",
      "Average Evaluation reward over 10 episodes: -0.37600000000000017\n",
      "Episode 4410, Average Evaluation Reward: -0.37600000000000017\n",
      "Average Evaluation reward over 10 episodes: -0.3700000000000002\n",
      "Episode 4420, Average Evaluation Reward: -0.3700000000000002\n",
      "Average Evaluation reward over 10 episodes: -0.6630000000000004\n",
      "Episode 4430, Average Evaluation Reward: -0.6630000000000004\n",
      "Average Evaluation reward over 10 episodes: -0.7990000000000005\n",
      "Episode 4440, Average Evaluation Reward: -0.7990000000000005\n",
      "Average Evaluation reward over 10 episodes: -0.6420000000000006\n",
      "Episode 4450, Average Evaluation Reward: -0.6420000000000006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mtrain_dqn\u001b[0;34m(agent, env, map_name, num_episodes, target_update, eval_every, seed)\u001b[0m\n\u001b[1;32m     14\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(agent\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state_tensor)\n\u001b[0;32m---> 16\u001b[0m next_state, reward, done, _, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m agent\u001b[38;5;241m.\u001b[39mbuffer_push(state, action, reward, next_state, done)\n\u001b[1;32m     19\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m, in \u001b[0;36mEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     28\u001b[0m     ret \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 29\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mle1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     30\u001b[0m     ret\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mle2\u001b[38;5;241m.\u001b[39minverse_transform(action\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mT[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     31\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mret))\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:158\u001b[0m, in \u001b[0;36mLabelEncoder.inverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 158\u001b[0m diff \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetdiff1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(diff):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(diff))\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.12/site-packages/numpy/lib/arraysetops.py:980\u001b[0m, in \u001b[0;36msetdiff1d\u001b[0;34m(ar1, ar2, assume_unique)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     ar1 \u001b[38;5;241m=\u001b[39m unique(ar1)\n\u001b[0;32m--> 980\u001b[0m     ar2 \u001b[38;5;241m=\u001b[39m \u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ar1[in1d(ar1, ar2, assume_unique\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)]\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.12/site-packages/numpy/lib/arraysetops.py:276\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m _unique1d(ar, return_index, return_inverse, return_counts, \n\u001b[1;32m    275\u001b[0m                     equal_nan\u001b[38;5;241m=\u001b[39mequal_nan)\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_unpack_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.12/site-packages/numpy/lib/arraysetops.py:125\u001b[0m, in \u001b[0;36m_unpack_tuple\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    121\u001b[0m     np\u001b[38;5;241m.\u001b[39msubtract(ary[\u001b[38;5;241m1\u001b[39m:], ary[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], result[l_begin:l_begin \u001b[38;5;241m+\u001b[39m l_diff])\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unpack_tuple\u001b[39m(x):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Unpacks one-element tuples for use as return values \"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rewards = train_dqn(agent, env, map_name=map_name, num_episodes=100000, eval_every=10, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2860e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
