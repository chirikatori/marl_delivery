{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2786006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import Environment\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41411fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state(state):\n",
    "    ret_state = {}\n",
    "    # state[\"time_step\"] = np.array([state[\"time_step\"]]).astype(np.float32).flatten(0)\n",
    "    # state[\"map\"] = np.array(state[\"map\"]).astype(np.float32)\n",
    "    ret_state[\"robots\"] = np.array(state[\"robots\"]).astype(np.float32).flatten()\n",
    "    ret_state[\"packages\"] = np.array(state[\"packages\"]).astype(np.float32).flatten()[:100]\n",
    "    if len(ret_state[\"packages\"]) < 1000:\n",
    "        ret_state[\"packages\"] = np.concatenate((ret_state[\"packages\"], np.zeros(100-len(ret_state[\"packages\"]))))\n",
    "    return np.concatenate(list(ret_state.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c5acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_shaping(r, env, state, action):\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5c206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env(gym.Env):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Env, self).__init__()\n",
    "        self.env = Environment(*args, **kwargs)\n",
    "\n",
    "        self.action_space = spaces.multi_discrete.MultiDiscrete([5, 3]*self.env.n_robots)\n",
    "\n",
    "        self.prev_state = self.env.reset()\n",
    "        first_state=convert_state(self.prev_state)\n",
    "        # Define observation space as a dictionary\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=100, shape=first_state.shape, dtype=np.float32)\n",
    "\n",
    "        from sklearn.preprocessing import LabelEncoder\n",
    "        self.le1, self.le2= LabelEncoder(), LabelEncoder()\n",
    "        self.le1.fit(['S', 'L', 'R', 'U', 'D'])\n",
    "        self.le2.fit(['0','1', '2'])\n",
    "\n",
    "    def reset(self, *args, **kwargs):\n",
    "        self.prev_state = self.env.reset()\n",
    "        return convert_state(self.prev_state), {}\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        return self.env.render()\n",
    "\n",
    "    def step(self, action):\n",
    "        ret = []\n",
    "        ret.append(self.le1.inverse_transform(action.reshape(-1, 2).T[0]))\n",
    "        ret.append(self.le2.inverse_transform(action.reshape(-1, 2).T[1]))\n",
    "        action = list(zip(*ret))\n",
    "\n",
    "        # You should not modify the infos object\n",
    "        s, r, done, infos = self.env.step(action)\n",
    "        new_r = reward_shaping(r, self.env, self.prev_state, action)\n",
    "        self.prev_state = s\n",
    "        return convert_state(s), new_r, \\\n",
    "            done, False, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f412a62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------add agent id------\n",
      "------use_orthogonal_init------\n",
      "------use_orthogonal_init------\n",
      "------set adam eps------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# rollout tối đa episode_limit bước\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m t \u001b[38;5;241m<\u001b[39m args\u001b[38;5;241m.\u001b[39mepisode_limit:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# Ở đây obs và state dùng cùng vector\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m()\n\u001b[1;32m     80\u001b[0m     actions, logps \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(obs, evaluate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     81\u001b[0m     obs_next, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from algorithms import MAPPO\n",
    "from algorithms.replay_buffer import ReplayBufferPPO as ReplayBuffer\n",
    "\n",
    "# 1. Khởi tạo môi trường wrapper\n",
    "map_file = \"map.txt\"  # đường dẫn tới file bản đồ\n",
    "n_robots = 5\n",
    "n_packages = 20\n",
    "max_time_steps = 100\n",
    "env = Env(\n",
    "    map_file=map_file,\n",
    "    n_robots=n_robots,\n",
    "    n_packages=n_packages,\n",
    "    max_time_steps=max_time_steps,\n",
    "    seed=2025\n",
    ")\n",
    "\n",
    "# 2. Tính toán kích thước đầu vào/đầu ra tự động từ env\n",
    "args = SimpleNamespace()\n",
    "args.N = env.env.n_robots\n",
    "# Actor input = quan sát vector từ env\n",
    "args.obs_dim = env.observation_space.shape[0]\n",
    "# Critic input = global state vector; ở đây wrapper trả về cũng chính là observation\n",
    "args.state_dim = args.obs_dim\n",
    "# Action dimension = số kết hợp move × pkg per agent\n",
    "factor_dims = env.action_space.nvec.reshape(env.env.n_robots, 2)\n",
    "args.action_dim = int(factor_dims[0, 0] * factor_dims[0, 1])\n",
    "\n",
    "# Siêu tham số huấn luyện\n",
    "args.episode_limit = max_time_steps\n",
    "args.rnn_hidden_dim = 64\n",
    "args.batch_size = 32\n",
    "args.mini_batch_size = 8\n",
    "args.max_train_steps = 100000\n",
    "args.lr = 3e-4\n",
    "args.gamma = 0.99\n",
    "args.lamda = 0.95\n",
    "args.epsilon = 0.2\n",
    "args.K_epochs = 4\n",
    "args.entropy_coef = 0.01\n",
    "args.set_adam_eps = True\n",
    "args.use_grad_clip = True\n",
    "args.use_lr_decay = True\n",
    "args.use_adv_norm = True\n",
    "args.use_rnn = False\n",
    "args.add_agent_id = True\n",
    "args.use_value_clip = True\n",
    "\n",
    "args.mlp_hidden_dim = 64\n",
    "args.rnn_hidden_dim = 64\n",
    "\n",
    "args.use_relu = False\n",
    "args.use_orthogonal_init = True\n",
    "\n",
    "\n",
    "# 3. Khởi tạo agent và replay buffer\n",
    "agent = MAPPO(args)\n",
    "buffer = ReplayBuffer(\n",
    "    args\n",
    ")\n",
    "\n",
    "# 4. Vòng lặp huấn luyện\n",
    "import time\n",
    "\n",
    "total_steps = 0\n",
    "while total_steps < args.max_train_steps:\n",
    "    buffer.reset_buffer()\n",
    "    for ep in range(args.batch_size):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        t = 0\n",
    "        obs_batch, state_batch, acts_batch, logps_batch, rews_batch, dones_batch = [], [], [], [], [], []\n",
    "\n",
    "        # rollout tối đa episode_limit bước\n",
    "        while not done and t < args.episode_limit:\n",
    "            # Ở đây obs và state dùng cùng vector\n",
    "            state = obs.copy()\n",
    "            actions, logps = agent.choose_action(obs, evaluate=False)\n",
    "            obs_next, reward, done, info = env.step(actions)\n",
    "\n",
    "            obs_batch.append(obs)\n",
    "            state_batch.append(state)\n",
    "            acts_batch.append(actions)\n",
    "            logps_batch.append(logps)\n",
    "            rews_batch.append(reward)\n",
    "            dones_batch.append(done)\n",
    "\n",
    "            obs = obs_next\n",
    "            t += 1\n",
    "\n",
    "        # Lấy giá trị cho mỗi bước trạng thái\n",
    "        values = agent.get_value(state_batch)\n",
    "        buffer.push(\n",
    "            obs_batch, state_batch, acts_batch,\n",
    "            logps_batch, rews_batch, dones_batch, values\n",
    "        )\n",
    "\n",
    "    # Cập nhật actor-critic\n",
    "    agent.train(buffer, total_steps)\n",
    "    total_steps += args.batch_size * args.episode_limit\n",
    "\n",
    "    # Lưu mô hình định kỳ\n",
    "    if total_steps % 10000 == 0:\n",
    "        agent.save_model(\"EnvCustom\", number=1, seed=2025, total_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4dfdf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
